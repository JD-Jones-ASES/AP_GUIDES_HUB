[
            { "name": "Statistics", "definition": "The science of collecting, organizing, analyzing, and interpreting numerical data to make informed decisions, like using survey data to predict election outcomes or medical studies to evaluate treatment effectiveness.", "unit": 1 },
            { "name": "Population", "definition": "The complete collection of all individuals or objects being studied, like all registered voters in the US or every light bulb produced by a factory—think of it as the 'whole universe' you want to understand.", "unit": 1 },
            { "name": "Sample", "definition": "A subset of the population actually examined to make inferences about the whole group, like polling 1,000 voters to predict how millions will vote—your window into the larger population.", "unit": 1 },
            { "name": "Parameter", "definition": "A numerical characteristic of a population, like the true average height of all American adults—usually unknown and what we're trying to estimate, denoted by Greek letters (μ, σ, π).", "unit": 1 },
            { "name": "Statistic", "definition": "A numerical characteristic calculated from sample data, like the average height from 100 randomly selected adults—our best guess at the parameter, denoted by Roman letters (x̄, s, p̂).", "unit": 1 },
            { "name": "Variable", "definition": "A characteristic that varies among individuals in a population, like height, gender, or test scores—the 'what' you're measuring or observing in your study.", "unit": 1 },
            { "name": "Quantitative Variable", "definition": "A variable measured numerically where arithmetic operations make sense, like height (5'8\") or temperature (72°F)—you can add, subtract, and find meaningful averages.", "unit": 1 },
            { "name": "Categorical Variable", "definition": "A variable that assigns individuals to groups or categories, like eye color (blue, brown, green) or college major—you can count frequencies but can't meaningfully average them.", "unit": 1 },
            { "name": "Discrete Variable", "definition": "A quantitative variable with countable values, often whole numbers, like number of children (0, 1, 2, 3...) or cars sold per day—gaps exist between possible values.", "unit": 1 },
            { "name": "Continuous Variable", "definition": "A quantitative variable that can take any value in an interval, like height (could be 5.83749... feet) or time—no gaps between possible values, limited only by measurement precision.", "unit": 1 },
           
            { "name": "Simple Random Sample (SRS)", "definition": "A sampling method where every possible sample of size n has an equal chance of selection, like drawing names from a hat—the gold standard that eliminates bias and enables valid inference.", "unit": 3 },
            { "name": "Stratified Random Sample", "definition": "Dividing the population into homogeneous groups (strata) then taking SRS from each, like separately sampling freshmen, sophomores, juniors, and seniors—ensures representation of all subgroups.", "unit": 3 },
            { "name": "Cluster Sample", "definition": "Randomly selecting entire groups (clusters) then surveying all members, like choosing 10 schools and surveying all students in those schools—convenient but may increase variability if clusters differ.", "unit": 3 },
            { "name": "Systematic Sample", "definition": "Selecting every kth individual after a random start, like choosing every 20th person from a list—simpler than SRS but can introduce bias if there's a hidden pattern.", "unit": 3 },
            { "name": "Convenience Sample", "definition": "Choosing individuals who are easily accessible, like surveying students in the cafeteria—quick and cheap but likely biased since it's not representative of the full population.", "unit": 3 },
            { "name": "Voluntary Response Sample", "definition": "Individuals choose to participate, often by calling in or responding to ads—strongly biased toward those with extreme opinions, like online polls or radio call-ins.", "unit": 3 },
            { "name": "Sampling Bias", "definition": "Systematic favoritism in sample selection that makes it unrepresentative, like only surveying people with landlines (missing cell-phone-only users)—threatens validity of conclusions.", "unit": 3 },
            { "name": "Undercoverage", "definition": "Some groups in the population are inadequately represented in the sample, like missing homeless people in a housing survey—leads to biased results.", "unit": 3 },
            { "name": "Nonresponse", "definition": "When selected individuals don't participate in the study, like 30% of people refusing to answer a political poll—can bias results if nonresponders differ systematically.", "unit": 3 },
            { "name": "Response Bias", "definition": "Tendency for responses to differ systematically from the truth due to question wording, interviewer effects, or social desirability—like people claiming to vote more than they actually do.", "unit": 3 },
           
            { "name": "Observational Study", "definition": "Research that observes individuals without imposing treatments, like studying the relationship between exercise and heart disease by following people's existing habits—can show association but not causation.", "unit": 3 },
            { "name": "Experiment", "definition": "Research where investigators deliberately impose treatments on subjects to observe responses, like giving some patients a new drug while others get placebo—only experiments can establish causation.", "unit": 3 },
            { "name": "Treatment", "definition": "A specific condition imposed on experimental subjects, like a particular drug dose or teaching method—the 'cause' you're testing to see its effect.", "unit": 3 },
            { "name": "Response Variable", "definition": "The outcome measured in a study, like blood pressure after treatment or test scores after instruction—what you think might be affected by the explanatory variable.", "unit": 3 },
            { "name": "Explanatory Variable", "definition": "The variable thought to influence the response, like type of medication or hours of study—the potential 'cause' in a cause-and-effect relationship.", "unit": 3 },
            { "name": "Confounding Variable", "definition": "A variable associated with both the explanatory and response variables that distorts their relationship, like age affecting both medication effectiveness and health outcomes—the 'lurking' variable that muddles conclusions.", "unit": 3 },
            { "name": "Randomization", "definition": "Using chance to assign treatments to subjects, ensuring that groups are similar except for the treatment—eliminates bias and allows causal conclusions, like flipping a coin to decide who gets the new drug.", "unit": 3 },
            { "name": "Control Group", "definition": "Subjects who receive a standard treatment or placebo for comparison, like patients getting current medication instead of the experimental drug—provides baseline to measure treatment effect.", "unit": 3 },
            { "name": "Placebo", "definition": "An inactive treatment given to control subjects, like sugar pills that look identical to real medication—controls for psychological effects of receiving treatment.", "unit": 3 },
            { "name": "Blinding", "definition": "Keeping subjects unaware of their treatment assignment to prevent bias, like patients not knowing if they're getting real medication or placebo—reduces placebo effect and response bias.", "unit": 3 },
            { "name": "Double-Blind", "definition": "Both subjects and researchers don't know treatment assignments until after data collection, preventing bias from either side—the gold standard for medical experiments.", "unit": 3 },
            { "name": "Replication", "definition": "Using many subjects in each treatment group to reduce chance variation, like testing a drug on 1000 patients rather than 10—increases reliability and power to detect real effects.", "unit": 3 },
           
            { "name": "Distribution", "definition": "The pattern of how values of a variable are spread out, showing which values occur and how often—like seeing that most students score 70-80 on a test with few very high or low scores.", "unit": 1 },
            { "name": "Mean", "definition": "The arithmetic average found by adding all values and dividing by count, sensitive to outliers—like if test scores are 85, 87, 90, 92, 96, the mean is 90.", "unit": 1 },
            { "name": "Median", "definition": "The middle value when data is ordered, resistant to outliers—in the scores 70, 75, 85, 90, 95, the median is 85 (middle value), unaffected by extreme scores.", "unit": 1 },
            { "name": "Mode", "definition": "The most frequently occurring value(s) in a dataset—if test scores include three 85s but only one or two of other scores, then 85 is the mode.", "unit": 1 },
            { "name": "Range", "definition": "The difference between maximum and minimum values, simple but sensitive to outliers—if test scores range from 65 to 98, the range is 33 points.", "unit": 1 },
            { "name": "Standard Deviation", "definition": "Average distance of data points from the mean, measuring spread—like if test scores have mean 80 and standard deviation 10, most scores fall between 70-90.", "unit": 1 },
            { "name": "Variance", "definition": "The square of standard deviation, measuring variability in squared units—if standard deviation is 10 points, variance is 100 square points, useful in calculations but harder to interpret.", "unit": 1 },
            { "name": "Interquartile Range (IQR)", "definition": "The range of the middle 50% of data (Q3 - Q1), resistant to outliers—if Q1 = 75 and Q3 = 90, then IQR = 15, showing the spread of typical values.", "unit": 1 },
            { "name": "Quartiles", "definition": "Values dividing ordered data into four equal parts: Q1 (25th percentile), Q2 (median, 50th), Q3 (75th)—like test score quartiles of 70, 80, 90 divide students into bottom, middle-low, middle-high, and top quarters.", "unit": 1 },
            { "name": "Percentile", "definition": "The value below which a certain percentage of data falls—scoring at the 85th percentile means you performed better than 85% of test-takers.", "unit": 1 },
            { "name": "Outlier", "definition": "An observation unusually far from the rest of the data, often defined as more than 1.5×IQR beyond the quartiles—like a test score of 40 when most scores are 75-95.", "unit": 1 },
            { "name": "Skewed Distribution", "definition": "A distribution where one tail is longer than the other—right-skewed has a long tail toward high values (like income), left-skewed toward low values (like age at death).", "unit": 1 },
           
            { "name": "Histogram", "definition": "A bar graph showing the distribution of quantitative data by grouping values into bins—like showing how many students scored 60-69, 70-79, 80-89, etc. on a test.", "unit": 1 },
            { "name": "Box Plot (Box-and-Whisker)", "definition": "A graph showing five-number summary (min, Q1, median, Q3, max) and outliers—reveals center, spread, skewness, and unusual values at a glance.", "unit": 1 },
            { "name": "Stem-and-Leaf Plot", "definition": "A display splitting each data value into a 'stem' and 'leaf'—like showing test scores where '8|3 5 7' represents scores of 83, 85, 87, preserving actual values while showing distribution.", "unit": 1 },
            { "name": "Dot Plot", "definition": "A simple graph with dots representing individual data points stacked above their values on a number line—useful for small datasets to see exact values and distribution shape.", "unit": 1 },
            { "name": "Bar Chart", "definition": "A graph showing frequencies or percentages for categorical data with separated bars—like showing how many students chose each college major, with bar heights representing counts.", "unit": 1 },
            { "name": "Scatterplot", "definition": "A graph plotting pairs of quantitative variables to show their relationship—like height vs. weight where each point represents one person, revealing correlation patterns.", "unit": 2 },
           
            { "name": "Probability", "definition": "The likelihood of an event occurring, expressed as a number between 0 and 1 (or 0% to 100%)—like P(rain tomorrow) = 0.3 means 30% chance of rain.", "unit": 4 },
            { "name": "Sample Space", "definition": "The set of all possible outcomes of a random phenomenon—like {H, T} for coin flip or {1, 2, 3, 4, 5, 6} for die roll, listing everything that could happen.", "unit": 4 },
            { "name": "Event", "definition": "A subset of the sample space, a collection of outcomes—like 'rolling an even number' = {2, 4, 6} when rolling a die, representing what you're interested in happening.", "unit": 4 },
            { "name": "Complement", "definition": "The event consisting of all outcomes NOT in the original event—if A = 'rain tomorrow', then A^c = 'no rain tomorrow', with P(A) + P(A^c) = 1.", "unit": 4 },
            { "name": "Union", "definition": "The event that occurs when either A or B (or both) happen, written A ∪ B—like 'getting an A in math OR English' includes getting an A in either or both subjects.", "unit": 4 },
            { "name": "Intersection", "definition": "The event that occurs when both A and B happen, written A ∩ B—like 'getting an A in math AND English' requires A's in both subjects.", "unit": 4 },
            { "name": "Mutually Exclusive", "definition": "Two events that cannot occur simultaneously—like rolling a 3 and rolling a 5 on the same die roll, if one happens the other cannot.", "unit": 4 },
            { "name": "Independent Events", "definition": "Events where the occurrence of one doesn't affect the probability of the other—like coin flips, where getting heads first doesn't change the probability of heads on the second flip.", "unit": 4 },
            { "name": "Conditional Probability", "definition": "The probability of event A given that event B has occurred, written P(A|B)—like probability of rain given cloudy skies, where additional information changes the likelihood.", "unit": 4 },
            { "name": "Law of Large Numbers", "definition": "As the number of trials increases, the observed relative frequency approaches the true probability—like flipping a fair coin many times will get closer to 50% heads.", "unit": 4 },
           
    
            { "name": "Random Variable", "definition": "A function assigning numerical values to outcomes of a random phenomenon—like X = sum of two dice, where each possible sum has a probability.", "unit": 4 },
            { "name": "Discrete Random Variable", "definition": "A random variable with countable values, often whole numbers—like number of correct answers on a 10-question quiz (0, 1, 2, ..., 10).", "unit": 4 },
            { "name": "Continuous Random Variable", "definition": "A random variable that can take any value in an interval—like exact height or time, where probabilities involve ranges rather than specific values.", "unit": 4 },
            { "name": "Expected Value", "definition": "The long-run average value of a random variable, calculated as Σ(x × P(x))—like the expected winnings from a lottery ticket considering all possible outcomes and their probabilities.", "unit": 4 },
            { "name": "Variance of Random Variable", "definition": "The expected squared deviation from the mean, measuring spread of a probability distribution—larger variance means more variable outcomes.", "unit": 4 },
            { "name": "Binomial Distribution", "definition": "The distribution of successes in n independent trials with constant probability p—like number of heads in 10 coin flips or correct guesses on a multiple-choice test.", "unit": 4 },
            { "name": "Geometric Distribution", "definition": "The distribution of the number of trials needed to get the first success—like how many coin flips until the first heads, where early success is more likely than late success.", "unit": 4 },
            { "name": "Normal Distribution", "definition": "A bell-shaped, symmetric distribution completely specified by mean μ and standard deviation σ—the most important distribution in statistics, modeling many natural phenomena like height and test scores.", "unit": 4 },
            { "name": "Standard Normal Distribution", "definition": "A normal distribution with mean 0 and standard deviation 1, used as a reference with z-scores—allows comparison of values from different normal distributions.", "unit": 4 },
            { "name": "z-score", "definition": "The number of standard deviations a value is from the mean, calculated as z = (x - μ)/σ—like a test score 2 standard deviations above average has z = 2.", "unit": 4 },
            { "name": "Empirical Rule (68-95-99.7)", "definition": "In normal distributions, approximately 68% of data falls within 1 standard deviation of the mean, 95% within 2, and 99.7% within 3—a quick way to understand normal distributions.", "unit": 4 },
           
     
            { "name": "Sampling Distribution", "definition": "The distribution of a statistic across all possible samples of the same size from a population—like the distribution of sample means if you took thousands of samples.", "unit": 5 },
            { "name": "Central Limit Theorem", "definition": "As sample size increases, the sampling distribution of the sample mean approaches normality regardless of the population shape—enables inference for any population with large enough samples (usually n ≥ 30).", "unit": 5 },
            { "name": "Standard Error", "definition": "The standard deviation of a sampling distribution, measuring variability of a statistic across samples—decreases as sample size increases, showing larger samples give more precise estimates.", "unit": 5 },
            { "name": "Sampling Distribution of Sample Mean", "definition": "Distribution of x̄ values from all possible samples of size n, with mean μ and standard deviation σ/√n—becomes more normal and less variable as n increases.", "unit": 5 },
            { "name": "Sampling Distribution of Sample Proportion", "definition": "Distribution of p̂ values from all possible samples of size n, approximately normal when np ≥ 10 and n(1-p) ≥ 10—the foundation for proportion inference.", "unit": 5 },
           
     
            { "name": "Confidence Interval", "definition": "A range of plausible values for a population parameter with a specified confidence level—like '90% confident the true mean is between 75 and 85', meaning our method captures the true value 90% of the time.", "unit": 6 },
            { "name": "Confidence Level", "definition": "The probability that the confidence interval method captures the true parameter—95% confidence means if we repeated the process many times, 95% of intervals would contain the true value.", "unit": 6 },
            { "name": "Margin of Error", "definition": "Half the width of a confidence interval, representing the maximum likely error in point estimate—larger margins indicate less precise estimates, affected by confidence level and sample size.", "unit": 6 },
            { "name": "Critical Value", "definition": "The z* or t* value used in confidence interval formulas, determined by the confidence level—like z* = 1.96 for 95% confidence, marking the boundary for the middle area under the curve.", "unit": 6 },
            { "name": "t-distribution", "definition": "A bell-shaped distribution used when population standard deviation is unknown, with slightly more spread than normal—approaches normal as sample size increases, with different curves for different degrees of freedom.", "unit": 7 },
            { "name": "Degrees of Freedom", "definition": "The number of values free to vary in a calculation, typically n-1 for one-sample t-procedures—determines which t-distribution curve to use, with more df closer to normal.", "unit": 7 },
            { "name": "Conditions for Confidence Intervals", "definition": "Requirements that must be met for valid inference: randomness, normality (or large sample), and independence—like checking that data comes from random sampling and sample size is adequate.", "unit": 6 },
           
      
            { "name": "Hypothesis Test", "definition": "A statistical procedure for making decisions about population parameters using sample data—like testing whether a new teaching method improves test scores compared to the standard method.", "unit": 6 },
            { "name": "Null Hypothesis (H₀)", "definition": "The claim of no effect or no difference, assumed true until evidence suggests otherwise—like H₀: μ = 50, representing the status quo or skeptical position.", "unit": 6 },
            { "name": "Alternative Hypothesis (Hₐ)", "definition": "The claim we suspect might be true and are trying to find evidence for—like Hₐ: μ > 50, representing what researchers hope to demonstrate.", "unit": 6 },
            { "name": "Test Statistic", "definition": "A standardized measure of how far sample data deviates from the null hypothesis—like t = (x̄ - μ₀)/(s/√n), with larger absolute values providing stronger evidence against H₀.", "unit": 6 },
            { "name": "p-value", "definition": "The probability of getting results as extreme or more extreme than observed, assuming H₀ is true—small p-values (typically < 0.05) suggest data is unlikely under H₀.", "unit": 6 },
            { "name": "Significance Level (α)", "definition": "The probability threshold for rejecting H₀, commonly 0.05—represents how much evidence we require, with smaller α requiring stronger evidence to reject the null.", "unit": 6 },
            { "name": "Type I Error", "definition": "Rejecting H₀ when it's actually true, like convicting an innocent person—probability equals α, the significance level we choose.", "unit": 6 },
            { "name": "Type II Error", "definition": "Failing to reject H₀ when it's actually false, like acquitting a guilty person—probability denoted β, harder to control than Type I error.", "unit": 6 },
            { "name": "Power", "definition": "The probability of correctly rejecting a false H₀, calculated as 1 - β—represents the test's ability to detect an effect when one exists, increased by larger samples or effect sizes.", "unit": 6 },
            { "name": "One-tailed vs Two-tailed Test", "definition": "One-tailed tests for directional alternatives (μ > 50 or μ < 50), two-tailed for non-directional (μ ≠ 50)—affects critical values and p-value calculation.", "unit": 6 },
           
         
            { "name": "One-sample z-test", "definition": "Hypothesis test for population mean when σ is known and population is normal (or n ≥ 30)—uses standard normal distribution for p-values and critical values.", "unit": 7 },
            { "name": "One-sample t-test", "definition": "Hypothesis test for population mean when σ is unknown—more common than z-test, uses t-distribution with n-1 degrees of freedom.", "unit": 7 },
            { "name": "Two-sample t-test", "definition": "Compares means from two independent groups—like comparing average test scores between two teaching methods, assuming both populations are normal with unknown standard deviations.", "unit": 7 },
            { "name": "Paired t-test", "definition": "Compares means from matched pairs or before/after measurements on same subjects—like comparing blood pressure before and after treatment for the same patients.", "unit": 7 },
            { "name": "One-proportion z-test", "definition": "Hypothesis test for population proportion when np₀ ≥ 10 and n(1-p₀) ≥ 10—like testing whether more than 50% of voters support a candidate.", "unit": 6 },
            { "name": "Two-proportion z-test", "definition": "Compares proportions from two independent groups—like comparing success rates between treatment and control groups, when sample sizes are large enough.", "unit": 6 },
            { "name": "Chi-square Test for Independence", "definition": "Tests whether two categorical variables are related in a population—like testing whether gender and college major choice are independent, using observed vs. expected frequencies.", "unit": 8 },
            { "name": "Chi-square Goodness of Fit", "definition": "Tests whether observed data follows a specific distribution—like testing whether a die is fair by comparing observed rolls to expected uniform distribution.", "unit": 8 },
           
   
            { "name": "Correlation (r)", "definition": "A measure of linear association between two quantitative variables, ranging from -1 to +1—values near ±1 indicate strong linear relationships, near 0 indicates weak relationships.", "unit": 2 },
            { "name": "Linear Regression", "definition": "A method for modeling the relationship between a response variable and explanatory variable using a straight line—like predicting height from shoe size using the equation ŷ = a + bx.", "unit": 2 },
            { "name": "Least Squares Regression Line", "definition": "The line that minimizes the sum of squared residuals, providing the 'best fit' through data points—like finding the line that makes prediction errors as small as possible overall.", "unit": 2 },
            { "name": "Slope (b)", "definition": "The rate of change in the regression line, showing how much y increases for each unit increase in x—like if b = 2.5, then each additional inch of height corresponds to 2.5 more pounds on average.", "unit": 2 },
            { "name": "y-intercept (a)", "definition": "The predicted y-value when x = 0 in the regression equation—like the predicted weight for someone with 0 height, often not meaningful but mathematically necessary.", "unit": 2 },
            { "name": "Residual", "definition": "The difference between observed and predicted values (y - ŷ)—positive residuals are above the line, negative below, showing prediction errors.", "unit": 2 },
            { "name": "Coefficient of Determination (r²)", "definition": "The proportion of variability in y explained by x, calculated as the square of correlation—like r² = 0.64 means 64% of height variation is explained by shoe size.", "unit": 2 },
            { "name": "Residual Plot", "definition": "A scatterplot of residuals vs. explanatory variable or predicted values—should show random scatter if linear model is appropriate, patterns suggest problems.", "unit": 2 },
            { "name": "Outlier in Regression", "definition": "A point that doesn't follow the overall pattern, with a large residual—like someone much taller than predicted by their shoe size.", "unit": 2 },
            { "name": "Influential Point", "definition": "A point that significantly changes the regression line if removed—usually has extreme x-value and pulls the line toward itself.", "unit": 2 },
            { "name": "Extrapolation", "definition": "Using the regression line to predict outside the range of x-values in the data—dangerous because the relationship might not continue beyond observed values.", "unit": 2 },
            { "name": "Lurking Variable", "definition": "An unobserved variable that influences both x and y variables, creating a misleading correlation—like how ice cream sales and drowning deaths both increase with temperature.", "unit": 2 }, 
	    { "name": "Five-Number Summary", "definition": "The minimum, first quartile (Q1), median, third quartile (Q3), and maximum values that provide a concise summary of a dataset's distribution—like encapsulating the spread and center of exam scores for quick comparison.", "unit": 1 },
{ "name": "Symmetric Distribution", "definition": "A distribution where the left and right sides mirror each other around the center, with the mean and median approximately equal—like the bell-shaped curve of standardized test scores in a large population.", "unit": 1 },
{ "name": "Bimodal Distribution", "definition": "A distribution featuring two distinct peaks or modes, often indicating two underlying groups—like heights in a sample combining basketball players and gymnasts.", "unit": 1 },
{ "name": "Relative Frequency", "definition": "The proportion or percentage of observations falling into a particular category or bin, calculated as frequency divided by total observations—like 25% of students preferring math over other subjects.", "unit": 1 },
{ "name": "Association", "definition": "The presence of a relationship between two variables, where changes in one tend to correspond with changes in the other—like observing that increased exercise is linked to lower blood pressure.", "unit": 2 },
{ "name": "Positive Association", "definition": "A relationship where higher values of one variable tend to pair with higher values of the other—like the link between years of education and income levels.", "unit": 2 },
{ "name": "Negative Association", "definition": "A relationship where higher values of one variable tend to pair with lower values of the other—like the connection between hours of TV watched and physical fitness scores.", "unit": 2 },
{ "name": "Causation", "definition": "A direct cause-and-effect relationship where changes in one variable produce changes in another—established through well-designed experiments, unlike mere correlation.", "unit": 2 },
{ "name": "Form of Association", "definition": "The overall pattern or shape of the relationship between variables in a scatterplot, such as linear, quadratic, or exponential—like identifying a straight-line trend in data on speed and distance.", "unit": 2 },
{ "name": "Direction of Association", "definition": "The trend of the relationship in a scatterplot, classified as positive (upward-sloping) or negative (downward-sloping)—like the positive direction between temperature and ice cream sales.", "unit": 2 },
{ "name": "Strength of Association", "definition": "The degree to which points in a scatterplot adhere to the overall form, ranging from weak (loose scatter) to strong (tight clustering)—quantified by the absolute value of the correlation coefficient.", "unit": 2 },
{ "name": "Residual Standard Deviation", "definition": "The standard deviation of the residuals in a regression model, measuring the typical prediction error—like if s = 5, most actual values are within about 5 units of the predicted line.", "unit": 2 },
{ "name": "Blocking", "definition": "Grouping experimental units that are similar in ways that might affect the response, then randomizing treatments within each group—like dividing plants by sunlight exposure before assigning fertilizers.", "unit": 3 },
{ "name": "Randomized Block Design", "definition": "An experimental setup that incorporates blocking to account for known variability, randomizing treatments within blocks—like testing teaching methods separately in each grade level.", "unit": 3 },
{ "name": "Matched Pairs Design", "definition": "A blocking technique where pairs of similar subjects are formed, and treatments are randomly assigned within each pair—like comparing two drugs by giving one to each twin in a pair.", "unit": 3 },
{ "name": "Addition Rule", "definition": "The formula for the probability of A or B: P(A ∪ B) = P(A) + P(B) - P(A ∩ B), adjusting for overlap—like the chance of drawing a red card or a king from a deck.", "unit": 4 },
{ "name": "Multiplication Rule", "definition": "For independent events, the probability of A and B: P(A ∩ B) = P(A) × P(B)—like the odds of rolling two sixes with fair dice being 1/6 × 1/6 = 1/36.", "unit": 4 },
{ "name": "Probability Distribution", "definition": "A model describing the possible values of a random variable and their associated probabilities—like listing the chances for each sum when rolling two dice.", "unit": 4 },
{ "name": "Standard Deviation of Random Variable", "definition": "The square root of the variance, measuring the typical deviation from the expected value—like for a die roll, it's about 1.71, showing the spread around the mean of 3.5.", "unit": 4 },
{ "name": "Census", "definition": "Collecting data from every member of the population, providing complete information but often impractical—like counting every citizen in a national survey.", "unit": 3 },
{ "name": "Chi-Square Test for Homogeneity", "definition": "A test to determine if the distribution of a categorical variable is the same across several populations or groups—like checking if snack preferences are consistent across different age groups.", "unit": 8 },
{ "name": "Expected Frequency", "definition": "The theoretical count in a chi-square test cell under the null hypothesis, computed as (row total × column total) / total sample size—like expecting 50 males and 50 females in a balanced group of 100.", "unit": 8 },
{ "name": "Chi-Square Statistic", "definition": "The sum of (observed - expected)^2 / expected across all categories, measuring deviation from the null hypothesis—like a value of 10 suggesting moderate evidence against uniformity.", "unit": 8 },
{ "name": "Population Slope (β)", "definition": "The true change in the response variable for each unit increase in the explanatory variable in the entire population, estimated by the sample slope b.", "unit": 9 },
{ "name": "t-Test for Regression Slope", "definition": "A hypothesis test assessing whether the population slope β differs from zero, indicating a linear relationship—like testing if study time significantly predicts exam scores.", "unit": 9 },
{ "name": "Confidence Interval for Regression Slope", "definition": "A range estimating the true population slope β with a given confidence level, like b ± t* × SE_b—capturing the plausible rates of change.", "unit": 9 },
{ "name": "Standard Error of the Slope", "definition": "The estimated standard deviation of the sample slope b across repeated samples, used in inference for β—like SE_b = 0.5 indicating the precision of the slope estimate.", "unit": 9 },
{ "name": "Conditions for Regression Inference", "definition": "Assumptions required for valid slope tests and intervals: linearity, independence, normality of residuals, and equal variance—like checking residual plots for random scatter without patterns.", "unit": 9 },
{ "name": "Homoscedasticity", "definition": "The condition where residuals have constant variance across all levels of the explanatory variable, essential for regression inference—like even spread in a residual plot versus predicted values.", "unit": 9 }
]